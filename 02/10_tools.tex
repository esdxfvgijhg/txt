
\subsubsection{Genetic algorithm}

\begin{frame}{Genetic algorithm}{}
	\Columns{.7}{.3}{
		\begin{itemize}
			\item $G= [g_{11}, ... , g_{nc}], ~~~~ g_{ij} \in [0,1]$, 1\leq i\leq n, 1\leq j\leq c.
			\begin{itemize}
				\item \textbf{n:} received packets,  \textbf{c:} applications.
			\end{itemize}
		\item $G_{0}= [g_{11}, ... , g_{nc}], ~~~~ g_{ij}=q_{ij} / \sum_{l=1}^{c} q_{il}$, \forall{i,j}.
			\begin{itemize}
			\item $q_{ij}=random(\mathbb{R})$ , \forall{i,j}.
			\end{itemize}
		\item $G_{t}= [g_{1}, ... , g_{n}]$
		\begin{itemize}
		\item \textbf{Selection (roulette wheel):}
		\begin{enumerate}
			\item $f(r_{i})=\beta(1-\beta)^{r_{i}-1}$
			\begin{itemize}
				\item $\beta \in [0,1]$  biggest section probability allowed.
				\item $r_{i} \in [1,n]$ rank of $g_{i}$
			\end{itemize}
			% \item R=$\{r_{x_{1}}, ..., r_{x_{n}}\}, r_{x_{e}} = random(\mathbb{R})$, 1\leq e\leq n,
			\item $\overline{g_{x}}=\{g_{x^{1}}, ..., g_{x^{z}}\}, F(r_{i-1}) \leq h_{x^{e}} \leq F(r_{i})$, 1\leq e\leq z\leq n, \forall i
			\begin{itemize}
				\item $h_{x^{e}} \in [0,1], h \sim Uniform$  %= random(\mathbb{R})$
				\item z: number of selected packets.
			\end{itemize}
		\end{enumerate}
		\item \textbf{Fitness/Crossover/Clustering:} 
		\Itemize{
			\item $\overline{g_{x}}$=FCM(\textbf{X})
		}
		\item \textbf{Mutation:} v \sim  $P$
		\Itemize{
			\item b is the mutation threshold (0.001).
		} \Acolade{\overline{g_{x}}}{
				\overline{g_{x}} & v \geq b \\
				\overline{q_{xj}} / \sum_{l=1}^{c} \overline{q_{xl}}, ~ \text{with} ~ \overline{q_{xj}}=random(\mathbb{R}) , \forall{j}. &  otherwise\\
		}


		% $g_{xj}=q_{xj} / \sum_{l=1}^{c} q_{xl}$, \forall{j}.
		% 	\begin{itemize}
		% 	\item $q_{xj}=random(\mathbb{R})$ , \forall{j}.
		% 	\end{itemize}
		\end{itemize}
		\end{itemize}
		}{
		  % \begin{tikzpicture}[node distance=1cm, every node/.style={fill=white, font=\sffamily,scale=0.8}, align=center]
	   %    \node (start)       [activityStarts]                     {Parameter initialization};
	   %    \node (fitness)     [selection, below of=start]          {Fitness function};
	   %    \node (crossover)   [selection, below of=fitness]        {Crossover};
	   %    \node (mutation)    [selection, below of=crossover]      {Mutation};
	   %    \node (selection)   [selection, below of=mutation]       {Survivor selection};
	   %    \node (end)         [activityStarts, below of=selection] {Ranked selection list};
	  
	   %    \draw[->]     (start)     -- (fitness);
	   %    \draw[->]     (fitness)   -- (crossover);
	   %    \draw[->]     (crossover) -- (mutation);
	   %    \draw[->]     (mutation)  -- (selection);
	   %    \draw[->]     (selection) -- (end);
	   %    \draw[->] (selection.east) to[bend right] (fitness.east);
	   %  \end{tikzpicture}
		}
 \end{frame}

 \begin{frame}

\Enumerate{
\item The Fuzzy C-Means Clustering algorithm takes in parameter a matrix of n packets received by the gateway by each end-devices with their p metrics (RSSI,
	ToA,
	BER,
	...).
\item The algorithm builds two other matrices,
	U which contains the membership degree of each packet to the c applications,
	and V, which contains the optimal p metrics that best fit the c applications.

\item The genetic algorithm starts by randomly generating a matrix with the same dimensions as matrix U.
\item The algorithm selects z packets and applies the Fuzzy C-Means Clustering algorithm on these z packets.

\item $\beta$ is a parameter that represents the biggest probability that a packet could have to be selected.
\item r is a parameter that defines its rank,
	a packet of rank 1 has the probability $\beta$ to be selected,
	a packet of rank r has a probability f(r) to be selected.
\item F(r) is the cumulative function of f(r), 
\item To select a set of packets to be sent to th FCM,
	a random variable between 0 and 1 is generated for each packet received.
	The probability to select packets decrease progressively until achieving all the packets.
\item Once the selected packets are chosen, we apply the FCM algorithm on the selected packets.
}

 \end{frame}

\subsubsection{Game Theory}
\begin{frame}{Game theory}{}
	\begin{itemize}
		\item $P= \{p_{1}, ... , p_{n}\}, ~~~~ p_{i} \in \mathbb{N}, 1\leq i\leq n.$
		\begin{itemize}
			\item n: number of players.
		\end{itemize}
		\item $S =\{s_{1}, ...,  s_{n}\}, 1\leq i\leq n$ 
		\begin{itemize}
			\item $s_{i}$ is the strategy set of the $i^{th}$ player.
		\end{itemize}
		\item $r_{i} (s_{i} , s_{-i})$ = $u_{k} : S \longrightarrow R_{+}$
		\begin{itemize}
			\item $s_{-i}=\left(s_{1}, \dots, s_{i-1}, s_{i+1}, \ldots, s_{n}\right) \in S_{1} \times \ldots \times S_{i-1} \times S_{i+1} \times \ldots \times S_{n}$
		\end{itemize}
	\end{itemize}
 \end{frame}


%\subsubsection{Matching Theory}



\subsubsection{Multi-Arm Bandits}
\begin{frame}{Multi-Armed-Bandit Algorithm}{}

%We compare the simulated standard ADR algorithm with 7 different multi-armed bandit algorithms:

%\Itemize{
%\item UCB [10] and
%\item (TS) Thompson Sampling [11] designed for stationary environments, 
%\item (SWUCB) Sliding Window UCB  [14],
%\item (STS) Switching Thompson Sampling  [15].
%\item (STSBA) Switching Thompson Sampling with Bayesian Aggregation  [16] designed for switching environments.
%\item EXP3 [17].
%\item REXP3 [20] for adversarial environments.
%}

 For each step $t=1, \dots, T$
\begin{itemize}
	\item \bf{Arms:} K = {1, ... , K}
	% \item \bf{Decision:} T = {1, ... , T}
	\item \bf{Reward:} $X^{k}_{t}$ with $\mu^{k}_{t}$ = E $[X^{k}_{t}]$
	\begin{itemize}
		\item Bernoulli rewards: $x_{k_{t}} \sim B\left(\mu_{k_{t}, t}\right)$
		\item \bf{Best reward:} $X^{*}_{t}$ with $\mu^{*}_{t}$ = max $\mu^{k}_{t}$,  k\in  K
		\item The reward $k_{t}$ is revealed $x_{k_{t}} \in[0,1]$
	\end{itemize}
\item  Minimize the pseudo regret:
\Itemize{
\item $R(T)=\sum_{t=1}^{T} \mu_{t}^{\star}-\mathbb{E}\left[\sum_{t=1}^{T} x_{k_{t}}\right]$

\item  where 
\Itemize{
\item $\mu_{t}^{\star}=\max _{k} \mu_{k, t}$
}
}
\end{itemize}

\end{frame}

\begin{frame}{Bandit Algorithm}{}

Growing number of Thompson Sampling f i,t : 

\Itemize{
	\item i denotes the starting time
	\item t the current time.
}

Let P(f i,t) be the probability at time t of the Thompson sampling starting at time i.

\medskip
\medskip


\Itemize{
\item Initialization: $\mathbb{P}\left(f_{1,1}\right)=1, t=1, \quad 2$
\Itemize{
\item $\forall k \in K \alpha_{k, f_{1,1}} \leftarrow \alpha_{0}, \beta_{k, f_{1,1}} \leftarrow \beta_{0}$
}


\item  Decision process: at each time t:

\Itemize{
\item $\forall i<t, \forall k: \theta_{k, f_{i, t}} \sim \operatorname{Beta}\left(\alpha_{k, f_{i, t}}, \beta_{k, f_{i, t}}\right) \quad$

\item Play (Bayesian Aggregation):
\Itemize{
\item $k_{t}=\arg \max _{k} \sum_{i<t} \mathbb{P}\left(f_{i, t}\right) \theta_{k, f_{i, t}} \quad$
}
}


\item  Instantaneous gain update:
\Equation{dd3}{\quad \forall i<t \mathbb{P}\left(x_{t} | f_{i, t}\right)=\left\{\begin{array}{ll}
{\frac{\alpha_{k, f_{i, t}}}{\beta_{k, f_{i, t}}+\alpha_{k, f_{i, t}}}} & {\text { if } x_{k_{t}}=1} \\
{\frac{\beta_{k, f_{i, t}}+\alpha_{k, f_{i, t}}}{\beta_{k, f_{i, t}}+\alpha_{k, f_{i, t}}}} & {\text { if } x_{k_{t}}=0}
\end{array}\right.}





\item Arm hyper parameters update:
\Equation{dd5d}{
\forall i<t\left\{\begin{array}{ll}
{\alpha_{k, f_{i, t}}=\alpha_{k, f_{i, t}}+1} & {\text { if } x_{k_{t}}=1} \\
{\beta_{k, f_{i, t}}=\beta_{k, f_{i, t}}+1} & {\text { if } x_{k_{t}}=0}
\end{array}\right.}




\item Distribution of experts update:

\Itemize{
\item $\forall i<t, \mathbb{P}\left(f_{i, t}\right) \propto(1-\rho) \cdot \mathbb{P}\left(x_{t} | f_{i, t-1}\right) \cdot \mathbb{P}\left(f_{i, t-1}\right)$
\item $f_{t, t}: \mathbb{P}\left(f_{t, t}\right) \propto \rho \sum_{i=1}^{t-1} \mathbb{P}\left(x_{t} | f_{i, t-1}\right) \cdot \mathbb{P}\left(f_{i, t-1}\right)$
\item $\alpha_{k, f_{t, t}}=\alpha_{0}, \beta_{k, f_{t, t}}=\beta_{0}$
}
}

\end{frame}




	\begin{frame}{Bandit Algorithm}{}

\Itemize{

\item \textbf{THOMPSON SAMPLING (TS)}
\Itemize{
\item success counter: $\alpha_{k}=\#\left(x_{k_{t}}=1\right)+\alpha_{0}$
\item failure counter: $\beta_{k}=\#\left(x_{k_{t}}=0\right)+\beta_{0}$
\item At each $t:$
\Itemize{
\item  $\theta_{k} \sim \operatorname{Beta}\left(\alpha_{k}, \beta_{k}\right)$
\item  $k_{t}=\arg \max _{k} \theta_{k}$
\item  $\left\{\begin{array}{ll}
{\alpha_{k}=\alpha_{k}+1} & {\text { if } x_{k_{t}}=1} \\
{\beta_{k}=\beta_{k}+1} & {\text { if } x_{k_{t}}=0}
\end{array}\right.$
}
}

\item \textbf{SWITCHING ENVIRONMENT}
\Equation{c1}{\mu_{k, t}=\left\{\begin{array}{ll}
{\mu_{k, t-1}} & {\text { probability } 1-\rho} \\
{\mu_{n e w} \sim U(0,1)} & {\text { probability } \rho}
\end{array}\right.}
}

\end{frame}




\subsubsection{Q-Learning}
\begin{frame}{Q Learning}{}
%	\Figure{h}{.5}{qlearning}{qlearning}
	$\red{Q\left(s_{t+1}, a_{t}\right)}=Q\left(s_{t}, a_{t}\right)+\gamma\left(\blue{R\left(s_{t}, a_{t}\right)}-Q\left(s_{t}, a_{t}\right)\right)$

	\Itemize{
	\item $\red{Q\left(s_{t+1}, a_{t}\right)}$ = new Q-Value
	\item $Q\left(s_{t}, a_{t}\right)$ = old Q-Value
	\item $\gamma$ = learning constant
	\item $\blue{R\left(s_{t}, a_{t}\right)}$ = immediate reward received after executing action a in state s at time t
	}
\end{frame}


\subsubsection{Marcov Chain}

\begin{frame}{Marcov chain}{}

\begin{flushleft}
\begin{equation}
V(s, \pi)=\mathbb{E}_{s}^{\pi}\left(\sum_{k=0}^{\mathrm{inf}} \gamma^{k} \cdot r\left(s_{k}, a_{k}\right)\right), s \in \mathbb{S}
\end{equation}

\begin{equation}
r\left(s_{k}, a_{k}\right)=G_{k} \cdot P R R\left(a_{k}\right)
\end{equation}

\begin{equation}
\pi^{*}=\arg \max _{\pi} V(s, \pi)
\end{equation}

% BER=10^{\alpha e^{\beta SNR}}


\end{flushleft}

\end{frame}

\begin{frame}{Marcov chain}{}

\Columns{0.6}{.4}{

Learning iterative steps:

\Itemize{
	\item \textbf{Choose} action $a_{k}(t) \sim \pi_{k}(t)$
	\item \textbf{Observe} game outcome 
	\Itemize{
		\item $a_{\_k}(t)$
		\item $u_{k}(a_{k}(t), a_{\_k}(t))$
	}
	\item \textbf{Improve} $\pi_{k}(t+1)$
}

Thus, we can expect that \forall k \in K

\Equation{e1}{\pi_{k(t)} \xrightarrow{t\longrightarrow\infty} \pi^{*}_{k}}
\Equation{e2}{u_{k}(\pi_{k}(t), \pi_{\_k}(t)) \xrightarrow{t\longrightarrow\infty} u_{k}(\pi^{*}_{k}, \pi^{*}_{\_k})}

Where:
\Itemize{
	\item $ \pi^{*} = (\pi^{*}_{1}, ..., \pi^{*}_{k})$ is the NE strategy profile
}

}{
	%\Figure{h}{1}{marcov}{}
}

\end{frame}

 
% \subsubsection{MCDM}

% \begin{frame}{Multi criteria decision making}{Background}

% \Columns{.35}{.65}{
% 	\Itemize{
% 		\item Configuration parameters:
% 		\Itemize{
% 			\item \ac{SF}
% 			\item \ac{CR}
% 			\item \ac{P^{tx}}
% 			\item \ac{BW}
% 			\item \ac{PS}
% 		}
% 		\item Configuration metrics:
% 		\Itemize{
% 			\item \ac{DR}
% 			\item \ac{PDR}
% 			\item \ac{RTD}
% 			\item \ac{ToA}
% 		}
% 	}
% }{

% \Matrix{Q n,m}{
% 		\                   & App~1    & \ldots  & App~M  \cr 
% 		C~1                 & q_{11}      & \ldots  & q_{1M}    \cr
% 		C~2                 & q_{21}      & \ldots  & q_{2M}    \cr
% 		~~\vdots            & \vdots      & \ddots  & \vdots    \cr    
% 		C~N                 & q_{N1}      & \ldots  & q_{NM}    \cr		
% }
% }
%  \end{frame}


% \begin{frame}{Genetic Algorithm}{}
% % \begin{itemize}
% 	% \item Evaluation function
% 		\begin{tabular}{ll}
% 			\multicolumn{2}{l}{\textbf{Evaluation function}} \\\\
% 			Define the number of parameters                       	& \{SF, Tx, CR, BW\} \\
% 			Define the target QoS			                      	& \{RSSI, SNR, delay, PDR, RTD\} \\
% 			Define evaluation function                            	& \red{Score}(SF, Tx, CR, BW) -> \{RSSI, SNR, delay, PDR, RTD\} \\\\
% 			\multicolumn{2}{l}{\textbf{Parameters}} \\\\
% 			Define a population of individuals (solutions)      	& 6720  \\
% 			Define probabilities of crossing and mutating       	& 0.5, 0.2 \\ 
% 			Define the number of generations                   	 	& 60 \\\\
% 			\multicolumn{2}{l}{\textbf{Generations}} \\\\
% 			Select individuals randomly    							& \{$SF_{i}, Tx_{i}, CR_{i}, BW_{i}\}^{random}$ 	\\
% 			Clone, crossover and mutate this individuals 	        & \{$SF_{i+1}, Tx_{i+1}, CR_{i+1}, BW_{i+1}\}^{random}$ \\
% 			Evaluate the offspring with an invalid Fitness 			& \red{Score}($SF_{i+1}, Tx_{i+1}, CR_{i+1}, BW_{i+1}$) \\\\
% 			\multicolumn{2}{l}{\textbf{(Crossover, Mutation)}} \\\\
% 			Remove some bad solutions                                & \\
% 			Duplicate some good solutions                            & \\
% 			Make small changes to some of them                       & \\
% 		\end{tabular}
% \end{frame}



\subsubsection{Best Response Dynamics}
\subsubsection{Fictitious Paly}
\subsubsection{Reinforcement Learning}
\subsubsection{Joint Utility Strategy}
\subsubsection{Trial and Error Learning}
\subsubsection{Regret Matching Learning}
\subsubsection{Imitation Learning}

\subsubsection{Clustering}
\begin{frame}{Clustering}{}

\footnotesize
%\Table{lllllll}{clustering}{clustering algorithms}{\\
\begin{tabular}{lllllll}
	\                       &  mutual info & v measure & adjusted rand & completeness & fowlkes mallows & homogeneity  \\\hline
	Agglomerative           &                 &           &               &              &                 &           \\
	MeanShift               &                 &           &               &              &                 &           \\
	AffinityProp            &                 &           &               &              &                 &           \\
	DBSCAN                  &                 &           &               &              &                 &           \\
	OPTICS                  &                 &           &               &              &                 &           \\
	FeatureAgg              &                 &           &               &              &                 &           \\
	Spectral                &                 &           &               &              &                 &           \\
	MiniKMeans              &                 &           &               &              &                 &           \\
	KMeans                  &                 &           &               &              &                 &           \\
	SpectralBi              &                 &           &               &              &                 &           \\
	SpectralCo              &                 &           &               &              &                 &           \\
	Birch                   &                 &           &               &              &                 &           \\
	FCM                     &                 &           &               &              &                 &           \\\hline
\end{tabular}

\end{frame}